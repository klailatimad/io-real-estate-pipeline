
# ğŸ—ï¸ Ontario Infrastructure Ontario (IO) Properties â€“ Data Pipeline PoC

A **local-first data engineering project** to ingest, clean, and prepare Ontarioâ€™s public **Infrastructure Ontario â€œProperties for Saleâ€** dataset for analysis and visualization.  
The goal is to design and practice a full **end-to-end data pipeline** â€” from ingestion and transformation to dashboarding â€” using modern open-source data tools.

----------

## ğŸš€ Project Overview

This project demonstrates how to:

-   **Ingest** real estate listings from a public ASP.NET WebForms site using Python and BeautifulSoup
-   Handle **pagination and form postbacks** (`__VIEWSTATE`, `__EVENTVALIDATION`)
-   **Normalize and clean** fields (price, acres, square footage, posted date, etc.)   
-   **Snapshot** raw data daily as CSVs for incremental history
-   **Transform** data with dbt + DuckDB into facts, dims, and marts
-   **Visualize** trends and insights in a **Streamlit dashboard**

All processing runs **locally** for compliance, reproducibility, and easy iteration.  
Future phases include validation (Great Expectations), orchestration (Airflow), and cloud migration.

----------

## ğŸ§± Repository Structure

```
real_estate_data_project/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/io_listings/ # Daily scraped CSVs (YYYY-MM-DD/io_listings.csv) 
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ target/io.duckdb # DuckDB warehouse 
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/ # stg_io_listings, stg_io_listings_all 
â”‚       â”œâ”€â”€ intermediate/ # int_io_events, int_io_latest_listing 
â”‚       â”œâ”€â”€ dims/ # dim_property, dim_location 
â”‚       â”œâ”€â”€ facts/ # fact_listing_daily, fact_listing_current 
â”‚       â””â”€â”€ marts/ # mart_price_trends, mart_source_quality 
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ set_env.sh # Exports IO_RAW_GLOB & IO_DUCKDB_PATH 
â”‚   â”œâ”€â”€ build_local.sh # Runs dbt build with env correctly set
â”‚   â”œâ”€â”€ dev_all.sh # Scrape -> dbt build -> Streamlit (local dev)
â”‚   â””â”€â”€ inspect_duckdb.py # Quick inspection & row counts 
â”‚
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ app.py # Streamlit dashboard (interactive filters, charts) 
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ io_scrape.py # Scraper module
â”‚   â”‚   â””â”€â”€ html_parser.py # HTML parsing helpers
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example # Example env vars for local setup
â””â”€â”€ README.md
``` 

----------

## âš™ï¸ Current Capabilities

|Feature|Description|
|---|---|
|**Web Scraper**|Extracts property listings from IOâ€™s â€œProperties for Saleâ€ portal|
|**Pagination Handler**|Handles ASP.NET `__doPostBack` pagination automatically|
|**Data Normalization**|Cleans and types numeric/date fields|
|**Daily Snapshots**|Saves CSVs under `data/raw/io_listings/YYYY-MM-DD/`|
|**dbt Transformations**|Builds layered models: staging â†’ facts/dims â†’ marts|
|**Incremental Model**|`fact_listing_daily` appends new daily records|
|**Validation Tests**|dbt tests ensure `not_null` and `unique` keys|
|**Streamlit Dashboard**|Interactive filters by Region, City, and Status|
|**Altair Charts**|Price distribution & weekly median price trends|
|**DuckDB Warehouse**|Local analytical database used by dbt + Streamlit|

----------

## ğŸ“Š Pipeline Workflow

### 1. Scrape Daily Listings

```sh
python -m src.ingestion.io_scrape \
  --out data/raw/io_listings \
  --page-size all \
  --sleep 1.0
```

â†’ Creates a new folder like:

`data/raw/io_listings/YYYY-MM-DD/io_listings.csv` 

----------

### 2. Run dbt Transformations

```sh
# From project root, set env so stg_io_listings_all can find all CSVs
export IO_RAW_GLOB="$(pwd)/data/raw/io_listings/*/io_listings.csv"

cd dbt
dbt run
dbt test
``` 
After a new daily scrape, you can force a full refresh of models that depend on all CSVs:

`dbt run --full-refresh --select stg_io_listings_all+ fact_listing_daily+`


Or just use the helper:

`./scripts/build_local.sh`

DBT Models:

-   `stg_io_listings_all` unions all daily CSVs
    
-   `fact_listing_daily` is **incremental**, appending new data daily
    
-   `fact_listing_current` and marts refresh fully
    

----------

### 3. Inspect DuckDB Data


```sh
# Optional: if you pointed IO_DUCKDB_PATH elsewhere
export IO_DUCKDB_PATH="dbt/target/io.duckdb"

python scripts/inspect_duckdb.py --db dbt/target/io.duckdb --limit 5
```

Displays:

-   Tables and row counts
    
-   Top rows for staging, facts, and marts
    
-   Region-level sample summaries
    

----------

### 4. Launch Streamlit Dashboard

```sh
# Ensure env is set so the app points to the correct DB
export IO_DUCKDB_PATH="dbt/target/io.duckdb"

streamlit run streamlit_app/app.py
``` 

**Features:**

-   Toggle between **Current** and **Historical** listings
    
-   Filter by Region, City, and Status
    
-   Choose date range (for Historical mode)
    
-   Metrics (listing count, median price/sqft/acres)
    
-   Charts (price distribution, weekly trends)
    
-   Full table of filtered results
    

----------

## ğŸ§© dbt Model Highlights

|Model|Type|Purpose|
|---|---|---|
|`stg_io_listings`|Table|Cleaned data from daily CSVs|
|`stg_io_listings_all`|View|Union of all CSV snapshots|
|`dim_location`|Table|City â†’ Region mapping|
|`dim_property`|Table|Property metadata|
|`fact_listing_daily`|Incremental|Historical records|
|`fact_listing_current`|Table|Latest state snapshot|
|`mart_price_trends`|Table|Weekly median prices by city|
|`mart_source_quality`|Table|Data completeness metrics|

----------

## ğŸ§ª Data Quality Tests

Run:

`dbt test` 

Tests include:

-   `unique` and `not_null` on `property_id`
    
-   `not_null` on `posted_date`
    
-   `unique` keys on facts and staging models
    

----------

## ğŸ“ˆ Visualization Highlights

-   **KPI cards** for listings, median price, acres, sqft
    
-   **Histogram** for price distribution
    
-   **Line chart** for weekly price trends
    
-   **Dynamic table** updated by sidebar filters
    
-   Works for both _Current_ and _Historical_ data views
    

----------

## ğŸ©º Troubleshooting

- dbt: â€œNo files found that match the pattern â€¦ io_listings.csvâ€
  - Set IO_RAW_GLOB to an absolute path, then rebuild:
    ```sh
    export IO_RAW_GLOB="$(pwd)/data/raw/io_listings/*/io_listings.csv"
    cd dbt && dbt run --full-refresh --select stg_io_listings_all+
    ```

- Streamlit shows empty data
  - Make sure youâ€™ve scraped and then run dbt. Also confirm `IO_DUCKDB_PATH` points to the right file.

- Date picker errors in Historical mode
  - Ensure you have at least one CSV day under `data/raw/io_listings/` and re-run dbt to populate `fact_listing_daily`.
----------

## ğŸªœ Next Steps

|Stage|Description|Status|
|---|---|---|
|**Data Validation**|Add Great Expectations or dbt data_tests|â³ Planned
|**Automation**|Airflow DAG for scrape + dbt + Streamlit|â³ Future|
|**Cloud Deployment**|Port to Snowflake or Azure Synapse|â³ Future|
|**Testing & CI/CD**|Add pytest and GitHub Actions|â³ Future
|**Dockerization**|Local containerized demo|â³ Future|

----------

## ğŸ§° Tools Used

-   **Python 3.11+** (`requests`, `beautifulsoup4`, `pandas`, `duckdb`, `altair`, `streamlit`)
    
-   **dbt-core 1.8+** (transformations, testing)
    
-   **DuckDB** (local analytical database)
    
-   **Streamlit** (interactive visualization)
    
-   **Altair** (charts)
    

----------
## ğŸ”§ Environment Variables

These are used by dbt (DuckDB) and the Streamlit app.

```sh
# REQUIRED: absolute path to your raw daily CSVs for stg_io_listings_all
export IO_RAW_GLOB="<ABSOLUTE_PATH>/data/raw/io_listings/*/io_listings.csv"

# OPTIONAL: override DuckDB file used by Streamlit & scripts
export IO_DUCKDB_PATH="dbt/target/io.duckdb"
```

- Linux/WSL/macOS:
    ```
    export IO_RAW_GLOB="$(pwd)/data/raw/io_listings/*/io_listings.csv"
    ```

- PowerShell:
    ```
    $env:IO_RAW_GLOB = "$(Get-Location)/data/raw/io_listings/*/io_listings.csv"
    ```

You can also run:
```sh
./scripts/set_env.sh
```

----------


## ğŸ§± Version History

|Branch|Description|Key Changes|
|---|---|---|
|`main`|Base ingestion and repo scaffold|Scraper + normalization logic|
|`feature/dbt-setup`|Added dbt project|dbt_project.yml, staging & dim models, tests|
|`feature/dbt-incremental`|Introduced incremental loads| `fact_listing_daily`,schema drift handling|
|`feature/dashboard`|Added Streamlit visualization|`app.py`, filters, charts, KPIs|
|`feature/dbt-path-fix`|Stablize dbt CSV glob and local dev flow|`IO_RAW_GLOB` env var for `stg_io_listings_all`, helper scripts (`set_env.sh`, `build_local.sh`, `dev_all.sh`), README updates|
|`main` (merged)|Current stable release|Fully working local pipeline from scrape â†’ dbt â†’ Streamlit|